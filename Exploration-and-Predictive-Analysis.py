# -*- coding: utf-8 -*-
"""“CIS545Project.ipynb”的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18-boHWwZQjjddASb9ZfSyz5VScZNogjG

# Exploration and Predictive Analysis of Cryptocurrency Trading with Binance History Data

## **Chapter 0**: Overview of this Project

In this section, we would like to share a bit of background knowledge concerning our project. We are insterested in the cyptocurrency market, a fast-growing financial market where investors from all round the world participated to trade on thousands of coin pairs. We shall use the [Kaggle dataset](https://www.kaggle.com/jorijnsmit/binance-full-history) of the historical prices of all coin pairs traded on [Binance](https://www.binance.us/en/home), which are represented as one-minute candlesticks. This is a typical time-series dataset that pertains to a variety of analytical techiques we have learned in the course. The following part will elucidate the value and importance of this project, and also answer questions including how our work solves problems that have not been solved before and why the result of our research is of great use in practice.

### **0.1** Difficulties of Financial Prediction & Common Pitfalls

Financial markets are generally difficult to predict. The logic behind this is quite simple: every participant wants to make money, and every one is trying to speculate in a lucrative way. Considering that there are millions of brilliant minds all attempting to make the right prediction and outsmart each other, it is not hard to see the erratic nature of financial market - Investors are always ready to annihilate any discovered patterns with their floods of trading orders.

Having said that, big institutional players in the financial market make decent predictions every year, usually with their own innovative models which are surely not as simple as a pure Neural Network or an ordinary Decision Tree. Our experience tells that the tools being used in practice are commonly many exquisite models stacked together, and financial institutions have a number of teams monitoring and tuning their models every day! 

When starting this project, I skimmed through some notebooks on Kaggle. While there are very few attempting to make predictions on this dataset, some notebooks seem to show quite satisfying prediction result. However, looking through these notebooks, I noticed two **mistakes** people have commonly made, which usually lead to a falsely statisfying result:

1. **Predict on prices rather than return rates**: The time series of asset prices is known to be non-stationary, which means it is not suitable for many classical statistical models. Moreover, if we directly do regression with this series, it is easy to achieve a extremely low level of relative error. After all, it is one-minute data and many asset prices do not change much within one minute (or even one day)! In other words, if we simply use the latest price as our next prediction, we are still likely to get a decent accuracy with the price sequence, but the model is basically useless in this case.

2. **Use unbalanced dataset**: Many notebooks like to slice the time series into 3 sub-sequences without any shuffling, and use them for training, validation & testing respectively. Nonetheless, a tiny bit of financial knowledge tells us that the market is often going through cycles of bullish periods and bearish periods. And thus, dividing up dataset by time will simply result in unbalanced samples. I have seen one paricular notebook that claims to achieve a over-90-percent accuracy, but when I checked its model, I found all the predictions are the same value of 1! Unbalanced samples can certainly be very problematic for both training and evalutation.

I mention these problems here not only to show the existence of common mistakes, but also, more importantly, to remind our readers that these are the errors that our project has identified and avoided. In the following sections, one will see how we bypass these pitfalls.

### **0.2** Practical Value of our Research

As shall be seen below, we make predictions on the **absolute value of logarithmic return rate**, which can sometimes be considered as an equivalent to the financial concept of **volatility** (Technically, a volatility is more often defined as the square of returns, but we do not want to introduce extraneous quadric factors into our data). The absolute value is a good indicator to capture the range of price fluctuations, and can be used in places where volatility calculation is involved, e.g., financial risk management, valuation of financial derivatives like options, etc. Since we know how volatility is extensively used in the financial world, we are also confident that the result of our model can be utilized in many scenarios during financial analysis.

### **0.3** Choice of Features & Labels

In this project, we will consider the logarithmic return of cryptocurrency prices. Given time $t$ and the closing price $P_t$, the logarithmic return $r_t$ is defined as: $r_t=log{P_t}-log{P_{t-1}}$. Transforming the price to the logarithmic return is a common practice in financial analysis. One major motive of such transformation is that the original price series is **theoretically non-stationary**: In many financial theories, $P_t$ is considered to follow something akin to a random walk (or more formally, a [geometric Brownian motion](https://en.wikipedia.org/wiki/Geometric_Brownian_motion)), and thus requires this transformation to become stationary.

We will use the **logarithmic return data from the last 10 minutes** along with the **current trading volume** and **current time** as **features**, to make predictions about the average volatility in the next 5 minutes, where the volatility is the absolute value of logarithmic return (as explained in the last section). Moreover, we will use the median of all 5-minute volatilities as a boundary to categorize the data (Note the *median* is used here to ensure the balancedness of data). Therefore, the **label** is **a Boolean value indicating whether the average volatility in the next 5 minutes is above median (1) or not (0)**. Now, we can have seen a classification problem, and it is time for some coding.

## **Chapter 1**: Prepare and Preprocess Training Data

First, we need to retrieve our data from [the Kaggle dataset](https://www.kaggle.com/jorijnsmit/binance-full-history). Before running the following code block, make sure that [kaggle.json](https://www.kaggle.com/docs/api) has been uploaded to the **/content** directory in Colab. The size of compressed data is around 20 GB, and it takes about 15 minutes to download and decompress the data. The raw data will be stored under the **/content/raw_data/** directory.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ! pip install kaggle
# ! mkdir ~/.kaggle
# ! cp kaggle.json ~/.kaggle/
# ! chmod 600 ~/.kaggle/kaggle.json
# ! kaggle datasets download jorijnsmit/binance-full-history
# ! mkdir -p /content/raw_data
# ! unzip -q binance-full-history.zip -d /content/raw_data
# ! mkdir -p /content/classification_data
# ! rm -f binance-full-history.zip

"""Let us see what these dataframes look like. There are totally **1,000 dataframes** corresponding to **1,000 trading pairs**. Here we display the contents of one dataframe (**AAVE-BTC.parquet**) as an example."""

import pandas as pd

pd.read_parquet('/content/raw_data/AAVE-BTC.parquet').head()

"""You will see an **EDA** of the downloaded data later. Now, let us first move on to preprocess the data. The raw data from Kaggle is already in good shape and does not suffer from missing values, so we only need to transform the format of data. As mentioned above, we use data from last 10 minutes to classify the average volatility in the next 5 minutes. We will shift the columns, and then put the corresponding features and labels in one row. We do this for all 1,000 dataframes, which takes several hours."""

import os
import numpy as np

files=os.listdir('/content/raw_data')
i=0
for i in range(len(files)):
  f=files[i]
  print('%d/%d: %s'%(i+1,len(files),f))
  X=pd.read_parquet('/content/raw_data/'+f).reset_index()
  X['year']=X['open_time'].dt.year
  X['weekofyear']=X['open_time'].dt.isocalendar().week.astype('int64')
  X['dayofweek']=X['open_time'].dt.dayofweek
  X['minuteofday']=X['open_time'].dt.hour*60+X['open_time'].dt.minute
  X['lg_rtn_lag0']=np.log(X['close']/X['close'].shift(1))

  lag=10
  for i in range(1,lag):
    X['lg_rtn_lag%d'%i]=X['lg_rtn_lag0'].shift(i)

  adv=5
  X['label']=np.abs(X['lg_rtn_lag0']).rolling(adv).mean().shift(-adv)
  med=X['label'].median()
  X['label']=X['label'].map(lambda x:1 if x>med else 0)

  droppedCols=['open_time','quote_asset_volume','number_of_trades','taker_buy_base_asset_volume','taker_buy_quote_asset_volume','open','close','high','low']
  featureCols=['volume','year','weekofyear','dayofweek','minuteofday','lg_rtn_lag0','lg_rtn_lag1','lg_rtn_lag2','lg_rtn_lag3','lg_rtn_lag4','lg_rtn_lag5','lg_rtn_lag6','lg_rtn_lag7','lg_rtn_lag8','lg_rtn_lag9']
  X=X.drop(columns=droppedCols).dropna()      # dropna() will remove the first 10 rows and the last 5 rows as expected
  X['features']=X[featureCols].values.tolist()
  X.drop(columns=featureCols).to_parquet('/content/classification_data/'+f,index=False)

"""Below is what a dataframe looks like after the preprocessing."""

pd.read_parquet('/content/classification_data/AAVE-BTC.parquet')

"""Now we will upload our preprocessed data to Amazon S3 bucket for later use. Before running the following code block, make sure the AWS CLI **credentials** file has been uploaded to the **/content** directory in Colab."""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ! pip install awscli
# ! mkdir ~/.aws
# ! cp -f /content/credentials ~/.aws/
# ! chmod 600 ~/.aws/credentials
# ! aws s3 cp --recursive /content/classification_data s3://cis545-group48/classification_data

"""## **Chapter 2**: Exploratory Data Analysis (EDA)

**EDA:** Exploratory data analysis is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.

For our data, we randomly choose one parquet file, the DOGE-BTC cryptocurrency pair, to show the EDA result. As for other files, we could apply the same methods and ideas to analyse the data.
"""

! pip install mplfinance

"""### **2.1** Data Loading and Preprocessing

#### **2.1.1** Loading parquet file
We randomly choose the file **DOGE-BTC.parquet** from the **Binance Full History | Kaggle** dataset. 

Load `DOGE-BTC.parquet` as `dog_df`.
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

dog_df = pd.read_parquet('/content/raw_data/DOGE-BTC.parquet')

"""Let's see the data in dataframe"""

dog_df

"""check the information of dog_df by using df.info()"""

dog_df.info()

"""#### **2.1.2** Add/Drop some columns
Drop the column `quote_asset_volume`,`taker_buy_base_asset_volume`,`taker_buy_quote_asset_volume`,`number_of_trades`. Store this into dog_clean_df.
"""

dog_clean_df=dog_df.drop(columns=['quote_asset_volume','taker_buy_base_asset_volume','taker_buy_quote_asset_volume','number_of_trades'])
dog_clean_df

"""From `open_time`, we can split the datestamps into some different parts, such as `year`, `month`, `date`, `time`, and `hour`"""

dog_clean_df = dog_clean_df.reset_index()
dog_clean_df['open_time'] = pd.to_datetime(dog_clean_df['open_time'])
dog_clean_df['year'] = dog_clean_df['open_time'].dt.year
dog_clean_df['month'] = dog_clean_df['open_time'].dt.month
dog_clean_df['date'] = dog_clean_df['open_time'].dt.date
dog_clean_df['time'] = dog_clean_df['open_time'].dt.time
dog_clean_df['hour'] = dog_clean_df['open_time'].dt.hour
dog_clean_df = dog_clean_df.set_index("open_time")
dog_clean_df

"""### **2.2** Mean, Standard Deviation, Min, and Max

Compute some simple statistics like mean, standard deviation, min and max for each of the following numerical features.

"""

info = dog_df.describe()
info

"""### **2.3** The regression relation between closing and opening prices
From the regplot, we can roughly draw a conclusion that in this data set, the opening and closing prices are reasonable, and a financial product's opening price should be the same as yesterday's closing price in the absence of important news. According to this chart, we can roughly see that there are few abnormal opening and closing events in this data set.
"""

sns.set(color_codes=True)
ax = sns.regplot(x="close", y="open", data=dog_df, line_kws={'color':'red'})

"""### **2.4** Visualization of price trend and simple Technical Analysis

#### **2.4.1** Visulization of open/high/low/close price of DOGE-BTC from all data

As this is a typical time varying data set, we used a broken line graph to visually display the relationship between the four types of historical prices and time. According to the graph, we can see:  The price before 2021 is about equal to 0, and there is no obvious fluctuation, so it is difficult to draw any meaningful conclusions. Therefore, we can further pay attention to the data of the past year, which has certain regularity and has certain timeliness.
"""

fig = plt.figure(figsize = (15,10))
plt.subplot(2, 2, 1)
plt.plot(dog_clean_df['open_time'], dog_clean_df['open'], color="red")
plt.title('DOGE-BTC Open Price')
plt.xticks(rotation=90)
plt.tight_layout()

plt.subplot(2, 2, 2)
plt.plot(dog_clean_df['open_time'], dog_clean_df['high'], color="black")
plt.title('DOGE-BTC High Price')
plt.xticks(rotation=90)
plt.tight_layout()

plt.subplot(2, 2, 3)
plt.plot(dog_clean_df['open_time'], dog_clean_df['low'], color="orange")
plt.title('DOGE-BTC Low Price')
plt.xticks(rotation=90)
plt.tight_layout()

plt.subplot(2, 2, 4)
plt.plot(dog_clean_df['open_time'], dog_clean_df['close'], color="green")
plt.title('DOGE-BTC Close Price')
plt.xticks(rotation=90)
plt.tight_layout()

plt.show()

"""#### **2.4.2** Visulization of open/high/low/close price of DOGE-BTC from past year (2021)

As we can see from the chart, prices did not rise or fall significantly in the first quarter of 2021, with a small spike in February. According to the news, on January 11, 2021, conventional financial markets were severely disrupted by the subordinated bond market known as WallStreetBets, or WSB.  The amateur stock traders decided to team up with traditional financial institutions and pull out some of the major hedge funds betting on stocks that had fallen in value, such as GameStop, BlackBerry, AMC, Nokia and Bed Bath&Beyond.  
 
On January 28, the WSB stock market mania spread to Dogecoin and fueled the biggest cryptocurrency rally to date. The rally was fueled in part by a number of tweets, including one from the world's richest man and avid Dogecoin fan Elon Musk.  

"""

dog_new_df = dog_clean_df.reset_index()
last1year_dog_df = dog_new_df[dog_new_df['open_time'] > '12-2020']

fig = plt.figure(figsize = (15,10))
fig.suptitle("Last 1 year open/high/low/close price of DOGE-BTC", fontsize=16)

plt.subplot(4, 1, 1)
plt.plot(last1year_dog_df['open_time'], last1year_dog_df['open'], color="red")
plt.legend(["open"])

plt.subplot(4, 1, 2)
plt.plot(last1year_dog_df['open_time'], last1year_dog_df['high'], color="black")
plt.legend(["high"])

plt.subplot(4, 1, 3)
plt.plot(last1year_dog_df['open_time'], last1year_dog_df['low'], color="orange")
plt.legend(["low"])

plt.subplot(4, 1, 4)
plt.plot(last1year_dog_df['open_time'], last1year_dog_df['close'], color="green")
plt.legend(["close"])

"""Since we can only get limited information from the line chart above, we try to use mplfinance, a professional financial instrument library, to plot the candlestick of prices in 2021, so as to make the data more meaningful.

In short, the candlestick can be read directly from **three factors**, each of which shows current market behavior and sentiment from a different perspective:  

**Factor 1: The physical size of the candlestick's entities**

Longer entities mean faster price fluctuations, such as buyers being more interested and therefore faster price increases; If the entity of the candlestick is strengthening over a period of time, then the price trend is also accelerating, the trend is strengthening; Smaller entities indicate an end to the current trend and a rebalancing of power between buyers and sellers; If the entity size is constant, the trend is stable.

**Factor 2: The length of the candlestick's shadow lines**

The length of the shadow lines helps us understand the volatility. Its characteristics are as follows: the long shadow line indicates high uncertainty, and buyers and sellers are wrestling fiercely, but no one side has an advantage; Short shadow line indicates stable market.

**Factor 3: the ratio of entities to shadow lines**

The above two factors need to be combined with this factor 3 to see the best:

In strong trends, entities are usually longer than shadows. The stronger the trend, the faster the price moves in the trend direction, and the shadow line is usually very short; As the trend slows down, the ratio of the entity to the shadow line changes, and the shadow line becomes longer.







"""

import mplfinance as fplt

mc = fplt.make_marketcolors(
                            up='tab:green',down='tab:red',
                            edge='lime',
                            wick={'up':'green','down':'red'},                     
                           )

s  = fplt.make_mpf_style(base_mpl_style="seaborn", marketcolors=mc, mavcolors=["black"])

Aweek = last1year_dog_df.set_index("open_time").groupby(pd.Grouper(freq = "W")).agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last'})

fplt.plot(
            Aweek,
            type= 'candle',
          style= s,
            title='Price Candlestick 2021 (groupby a week)',
          mav=7    
        )

"""#### **2.4.3** The relation between closing price and volume (2021)

By comparing the historical trend of the closing price and transaction volume of 2021 in a parallel way, we can clearly see that from January to April 2021, the closing price of Dogecoin in this period is still at a low level compared with the price of the whole year, although it has increased in general.  In contrast, the total volume of transactions during this period is the highest of the year.  Between May and July, the total volume of transactions began to fall sharply, and prices fell as well.  But from a macro point of view, when the price level is high, the trading volume is reduced, which is related to people's herd psychology and the strategy of "selling high and buying low".  


The reasons for this may be that：

(1) In the early days (From January to April), investors gradually become aware of dogecoin, and more people participate in it, which partly drives up the price, which in turn attracts a new group of investors, thus keeping the transaction volume high.  
(2) For some periods of time, different from the well-known strategy of "high selling and low buying", "the higher the price, the greater the trading volume" occurs, which fully indicates that investors are still full of confidence in Dogecoin and believe that its price still has a lot of room for rise, and fully recognize its prospects.  
(3) In the middle and late period (From May to July), the popularity of Dogecoin began to fade, which was related to the adverse news in the market, and some investors began to lose confidence in dogecoin and stopped holding it.  

"""

from datetime import datetime
from matplotlib.dates import date2num

last1year_dog_df = dog_new_df[dog_new_df['open_time'] > '12-2020']

fig= plt.figure(figsize = (15,10))
fig.suptitle("Last 1 year [close price] VS [Volume]", fontsize=16)

plt.subplot(2, 1, 1)
plt.plot(last1year_dog_df['open_time'], last1year_dog_df['close'], color="black")
plt.legend(["Close"])
plt.axvspan(date2num(datetime(2021,5,1)), date2num(datetime(2021,7,1)),color="green", alpha=0.3)

plt.subplot(2, 1, 2)
plt.plot(last1year_dog_df['open_time'] , last1year_dog_df['volume'])
plt.legend(['DOGE-BTC'])

plt.axvspan(date2num(datetime(2021,5,1)), date2num(datetime(2021,7,1)),color="green", alpha=0.3)

"""#### **2.4.4** Analyze with a simple moving average (SMA) trading strategy

In this section, we will attempt to visually assess the performance of a SMA crossover strategy. There are many ways to do this strategy, but we will go with a “price crossover” approach with a 30 day SMA.

In this case, it’s considered a “buy” signal when the closing price crosses the simple moving average from below, and considered a “sell” signal when the closing price crosses the simple moving average from above.

So how do we know our SMA price crossover strategy is effective? Visually, we can assess this by seeing if the “sell” signal happens right before the stock price starts going down, and if the “buy” signal happens right before the stock price starts going up.
"""

import mplfinance as fplt

mc = fplt.make_marketcolors(
                            up='tab:green',down='tab:red',
                            edge='lime',
                            wick={'up':'green','down':'red'},                     
                           )

s  = fplt.make_mpf_style(base_mpl_style="seaborn", marketcolors=mc, mavcolors=["black"])

SMA = dog_clean_df

SMA = SMA.groupby(pd.Grouper(freq = "15D")).agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last'})

fplt.plot(
            SMA,
            type= 'candle',
          style= s,
            title='A 30 Day SMA Trading Strategy',
          mav=2    
        )

"""#### **2.4.5** The rate of change between the closing and opening prices

According to the regression graph above, we can conclude that there is not much difference between the opening price and the closing price, but we want to further investigate the rate of change between the closing price and the opening price. If the graph is drawn at the interval of minutes, it is too dense and takes a long time to run, so we try to draw it at the interval of months.  As you can see, there are peaks in February and May 2021, respectively.  This also corresponds to the price chart we made above in 1.4.2.  But it is worth mentioning that from 1.4.2, we can see that May is the highest price of the year, while February is only a small peak.  But if you look at the rate of change, February's monthly increase was even bigger than May's.  This shows that when we invest, we should not only observe the price trend, but also consider the rate of change, so that we can make a more scientific decision.  

"""

month_df = dog_clean_df
month_df = month_df.groupby(pd.Grouper(freq = "M")).agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last'})

month_df['CHANGE (%)'] = (month_df['close'] - month_df['open']) / month_df['open'] * 100

sns.set(rc={'figure.figsize':(5,4)})

def display_changes(df):
    plt.xticks(rotation=45) 
    ax = sns.lineplot(data=df, x='open_time', y='CHANGE (%)')
    ax.set(xlabel='open_time', ylabel='change(%)')
    
display_changes(month_df)

"""### **2.5** Correlation of Feature Variables
Create a **correlation matrix** and a **correlation heatmap** for the features in `dog_df`. As can be seen from the heatmap, the values in the columns *open*, *high*, *low*, *close* are highly correlated (The correlations are not exactly 1, but shown as 1 because of **rounding**). This is why we discarded the columns *open*, *high*, *low* during the **preprocessing** of data.
"""

correlation_matrix = dog_df.corr()
correlation_matrix

plt.figure(figsize=(15,8))
sns.heatmap(correlation_matrix, annot=True)
plt.show()

"""## **Chapter 3**: Start testing the performance of different machine learning models

1. Logistic Regression
1. Decision Tree
1. Random Forest
1. ANN

### **3.1** Retrieve and Validate Data from S3 bucket

#### **3.1.1** Download the dataset from our amazon s3 bucket
"""

! wget https://cis545-group48.s3.amazonaws.com/classification_data/BTC-USDT.parquet

"""#### **3.1.2** Import essential Python libraries"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""#### **3.1.3** Read the dataset into a Pandas DataFrame"""

df = pd.read_parquet('BTC-USDT.parquet')

# Show the head of the data to check whether our data was loaded properly
df.head()

# To see how the data is distrubuted
# Noticed that all the features are arranged in a list stored in features column
df.describe()

# Rearrange the list into separate columns and assign them to X(features) y(labels)
X = np.stack(df.features.values)
y = df.label.values

"""#### **3.1.4** Label distribution

* As we can see, category 0 and category 1 are both at around 1e6 and category 0 is only 1 data more than category 1.
* If we randomly assign the label, our precision and recall will both yield at 50%. So, our model must have the precision or recall higher than 50%.

"""

# Plot the distribution of the labels
sns.countplot(y)

df.label.value_counts()

"""#### **3.1.5** Import some scikit-learn modules"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import confusion_matrix, classification_report

"""#### **3.1.6** Split the dataset into training data and validation data"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""#### **3.1.7** Normalize the data by feature scaling
* MinMaxScaler will do this job \
* $ x := \frac{x – min}{max – min}$
"""

scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
# check the data after 
X_train[:5]

"""### **3.2** Linear Model: Logistic Regression (Base line)
* Since linear model is one of the simplest model for machine learning, We trained a linear classifier to be our base line.

#### **3.2.1** Training
"""

from sklearn.linear_model import LogisticRegression

model_lg = LogisticRegression()
model_lg.fit(X_train, y_train)

pred1 = model_lg.predict(X_test)

"""#### **3.2.2** Result"""

print(confusion_matrix(y_test, pred1))

fig = sns.heatmap(confusion_matrix(y_test, pred1) / len(y_test), annot=True, fmt='.2%', cmap='Blues')
fig.axes.xaxis.set_ticks_position("top")

print(classification_report(y_test, pred1))

"""### **3.3** Decision Tree without pruning"""

from sklearn.tree import DecisionTreeClassifier

"""#### **3.3.1** Train"""

model_tree = DecisionTreeClassifier(criterion='entropy')
model_tree.fit(X_train, y_train)

pred2 = model_tree.predict(X_test)

"""#### **3.3.2** Result"""

print(confusion_matrix(y_test, pred2))

fig = sns.heatmap(confusion_matrix(y_test, pred2) / len(y_test), annot=True, fmt='.2%', cmap='Blues')
fig.axes.xaxis.set_ticks_position("top")

print(classification_report(y_test, pred2))

"""### **3.4** Pruning Decision Tree
* If we did not limit the depth of the decision tree, our model will easily become overfitting. One way to prevent overfitting is to prune the decision tree.
"""

# set a list of potentioal ccp alpha valuses
ccp_alphas = [0.0, 0.01, 0.02, 0.03, 0.05, 0.1]
tree_models = []
# iterate through them and found out a best one
for ccp_alpha in ccp_alphas:
    tree_testing = DecisionTreeClassifier(random_state=0, ccp_alpha=ccp_alpha)
    tree_testing.fit(X_train, y_train)
    tree_models.append(tree_testing)
print(
    "Number of nodes in the last tree is: {} with ccp_alpha: {}".format(
        tree_models[-1].tree_.node_count, ccp_alphas[-1]
    )
)

"""#### **3.4.1** Result
* As we can see from the diagram below, our original model was not overfitting at all.
* So, we can keep the original one
"""

train_scores = [tree_testing.score(X_train, y_train) for tree_testing in tree_models]
test_scores = [tree_testing.score(X_test, y_test) for tree_testing in tree_models]

fig, ax = plt.subplots()
ax.set_xlabel("alpha")
ax.set_ylabel("accuracy")
ax.set_title("Accuracy vs alpha for training and testing sets")
ax.plot(ccp_alphas, train_scores, marker="o", label="train", drawstyle="steps-post")
ax.plot(ccp_alphas, test_scores, marker="o", label="test", drawstyle="steps-post")
ax.legend()
plt.show()

"""#### **3.4.2** Tree visualization (depth=3))
* Since the depth of decision tree is very deep, we cannot show the entire view of it
* From the tree diagram, we can see that there is not a distinct feature which allow us to seperate some part of the data in just few steps
* No leaf node in just 3 layers
"""

from sklearn.tree import export_graphviz
import graphviz

tree_graph = export_graphviz(model_tree, filled=True, max_depth=3)

graphviz.Source(tree_graph)

"""### **3.5** Random Forest"""

from sklearn.ensemble import RandomForestClassifier

"""#### **3.5.1** Train"""

model_rf = RandomForestClassifier(n_estimators=20, criterion='entropy', max_depth=50, random_state=0)
model_rf.fit(X_train, y_train)

pred3 = model_rf.predict(X_test)

"""#### **3.5.2** Result"""

print(confusion_matrix(y_test, pred3))

fig = sns.heatmap(confusion_matrix(y_test, pred3) / len(y_test), annot=True, fmt='.2%', cmap='Blues')
fig.axes.xaxis.set_ticks_position("top")

print(classification_report(y_test, pred3))

"""### **3.6** ANN"""

import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import TensorBoard

"""#### **3.6.1** Show TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir ./logs #Launch tensorboard

"""#### **3.6.2** Check CUDA and GUP"""

! nvcc --version #check CUDA

!nvidia-smi -L #check GPU name

from tensorflow.python.client import device_lib 
print(device_lib.list_local_devices())

"""#### **3.6.3** Initialize some callbacks"""

early_stop = EarlyStopping(monitor='val_loss', patience=25, mode='min')
tensorboard_callback = TensorBoard(log_dir="./logs")

"""#### **3.6.4** Build the model"""

model_ann = Sequential()

model_ann.add(Dense(16, activation='relu'))
model_ann.add(Dense(64, activation='relu'))
model_ann.add(Dense(64, activation='relu'))
model_ann.add(Dense(128, activation='relu'))
model_ann.add(Dropout(0.5))
model_ann.add(Dense(64, activation='relu'))
model_ann.add(Dense(1, activation='sigmoid'))

model_ann.compile(optimizer='adam', loss='BinaryCrossentropy')

input_shape = X_train.shape
model_ann.build(input_shape=input_shape)
model_ann.summary()

"""#### **3.6.5** Train"""

with tf.device("gpu:0"):
  model_ann.fit(x=X_train, y=y_train, batch_size=256, epochs=200, verbose=1, callbacks=[early_stop, tensorboard_callback], validation_data=(X_test, y_test))

pred4 = model_ann.predict(X_test)

pred4_class = pred4 > 0.5

pred4_class = pred4_class.astype(int)

"""#### **3.6.6** Result"""

print(confusion_matrix(y_test, pred4_class))

fig = sns.heatmap(confusion_matrix(y_test, pred4_class) / len(y_test), annot=True, fmt='.2%', cmap='Blues')
fig.axes.xaxis.set_ticks_position("top")

print(classification_report(y_test, pred4_class))

model_ann.save("./model.h5")

"""After using different models and see how it performs, it may seem not satisfying as for the prediction result. Thus, we have something bigger to perform to improve the prediction.

## **Chapter 4**: Use XGBoost on Big Data with Apark Spark

We have seen multiple models, but many of them are not particularly parallelizable and will take a lot of time to be trained on the whole 20 gigabytes of data. In this section, we will employ the **XGBoost** model and apply it over the real **big data**.

The [XGBoost](https://en.wikipedia.org/wiki/XGBoost) model is based on the Decision Tree model which we have covered in class. It uses [gradient boosting](https://en.wikipedia.org/wiki/Gradient_boosting) to ensemble stumps and add some tweaks to prevent overfitting. Sadly, we do not have the space for a thorough elaboration of the XGBoost model here, but for the purpose of understanding this project, it is enough to know that XGBoost is quite suitable for distributed machine learning and Apache Spark can be equipped with the package *XGBoost4J* to train XGBosst models. *XGBoost4J* does not have any official support for Python, but we managed to find a [Python interface to *XGBoost4J*](https://github.com/sllynn/spark-xgboost) provided by a good Samaritan on GitHub.

There are two steps in the training: First, we tune the hyperparameters using from a portion of the dataframes. This gives us an idea of the rough range of hyperparameters. Then we move to the Colab environment and train the 1,000 XGBoost models for all of the 1,000 trading pairs, utilizing the rough estimation of hyperparameters we have obtained in Step 1. Due to the limited computational resources, we cannot fit the best models for all trading pairs. But as can be seen later, even with the roughly estimated hyperparameters, these trained models can still be of much practical use.

### **4.1** Tune Hyperparameters on Amazon EMR Cluster

Clearly, we need to build an Amazon Elastic MapReduce Cluster before moving on. We edited the *CloudFormation template* used in class to perform the following modifications:

1. The cluster now consists of 1 master nodes and 4 core nodes

2. By setting *livy.server.session.timeout* in the cluster configuration, we ensure that now a livy session can at most tolerate an 5-hour-long operation; This is needed for many time-consuming operations

3. By setting the *maximizeResourceAllocation* for Spark in the cluster configuration, we ensure most hardware resources in the cluster will be allocated for Apache Spark

4. We allow for using the instances of *c5.xlarge* and *c5a.xlarge*, which are computation-oriented instances with hardware specifications similar to *m5.xlarge*. Unfortunately, AWS does not permit more advanced instances, so this is as far as we could get

We also updated the *bootstrap script* to provide dependencies such as the [Python interface to *XGBoost4J*](https://github.com/sllynn/spark-xgboost).

Our [CloudFormation template](https://cis545-group48.s3.amazonaws.com/emr-course.yml) and [bootstrap script](https://cis545-group48.s3.amazonaws.com/update.sh) are both stored in the Amazon S3 bucket so that they can be downloaded conveniently.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ! apt update
# ! apt install gcc python-dev libkrb5-dev
# ! pip install sparkmagic

# Commented out IPython magic to ensure Python compatibility.
# %load_ext sparkmagic.magics

"""Note that we need to tell **Apache Spark** where to locate the *XBoost4J* package before establishing the session."""

# Commented out IPython magic to ensure Python compatibility.
# %%spark config
# {"jars":[
#          "https://repo1.maven.org/maven2/ml/dmlc/xgboost4j_2.12/1.3.1/xgboost4j_2.12-1.3.1.jar",
#          "https://repo1.maven.org/maven2/ml/dmlc/xgboost4j-spark_2.12/1.3.1/xgboost4j-spark_2.12-1.3.1.jar"
#          ]
# }

# Commented out IPython magic to ensure Python compatibility.
# %spark add -s spark_session -l python -u 	http://ec2-52-90-115-102.compute-1.amazonaws.com -a cis545-livy -p testpwd -t Basic_Access

# Commented out IPython magic to ensure Python compatibility.
# %spark info

"""Now we will fetch a list of **all** coin pairs by scanning the files in the **/classification_data** directory (which contains our preprocessed *.parquet* data files) of the S3 bucket. Then we will also fetch a list of **processed** coin pairs by scanning the folders in the **/xgboost_classification_model** directory (which contains our trained XGBoost models). And we can figure out those **unprocessed** coin pairs which need to be handled. This mechanism allows us to resume our progress from last running."""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# import boto3
# 
# s3 = boto3.client('s3')
# 
# prefix = 'classification_data/'
# suffix = '.parquet'
# 
# lp = len(prefix)
# ls = len(suffix)
# 
# coin_pairs = []
# for page in s3.get_paginator('list_objects_v2').paginate(Bucket='cis545-group48',Prefix=prefix,Delimiter='/'):
#     coin_pairs.extend([obj['Key'][lp:-ls] for obj in page.get('Contents',[]) if len(obj['Key'])>lp])
# 
# # coin_pairs contains a list of all coin pairs
# 
# prefix = 'xgboost_classification_model/'
# suffix = '/'
# 
# lp = len(prefix)
# ls = len(suffix)
# 
# processed = []
# for page in s3.get_paginator('list_objects_v2').paginate(Bucket='cis545-group48',Prefix=prefix,Delimiter='/'):
#     processed.extend([sub['Prefix'][lp:-ls] for sub in page.get('CommonPrefixes',[]) if len(sub['Prefix'])>lp])
# 
# # processed contains a list of processed coin pairs
# 
# unprocessed = list(set(coin_pairs)-set(processed))
# 
# print(processed)
# print(unprocessed)

"""The next step is to handle the unprocessed coin pairs one by one. A **grid search** is used to find the best model for the current dataframe (i.e., for the current coin pair). After this, we save the optimized model to the Amazon S3 bucket."""

# Commented out IPython magic to ensure Python compatibility.
# %%spark
# 
# from pyspark.ml.linalg import Vectors, VectorUDT
# from pyspark.sql.functions import udf
# from sparkxgb import XGBoostClassifier
# from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit
# from pyspark.ml.evaluation import BinaryClassificationEvaluator
# 
# list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())
# 
# for pair in unprocessed:
#   data_sdf = spark.read.load(path='s3://cis545-group48/classification_data/%s.parquet'%pair,format='parquet')
#   data_sdf = data_sdf.select(
#       data_sdf["label"],
#       list_to_vector_udf(data_sdf["features"]).alias("features")
#   )
# 
#   train_sdf, test_sdf = data_sdf.randomSplit([0.8, 0.2],seed=2021)
# 
#   xgb = (
#       XGBoostClassifier()
#     .setFeaturesCol('features')
#     .setLabelCol('label')
#   )
# 
#   param_grid = (
#     ParamGridBuilder()
#     .addGrid(xgb.numRound, [80,115,150])
#     .addGrid(xgb.maxDepth, [5,10])
#     .build()
#   )
# 
#   cv_model = TrainValidationSplit(
#     estimator=xgb,
#     estimatorParamMaps=param_grid,
#     evaluator=BinaryClassificationEvaluator(),
#     trainRatio=0.8,
#     parallelism=18
#   ).fit(train_sdf)
# 
#   xgb_model = cv_model.bestModel
# 
#   xgb_model.save('s3://cis545-group48/xgboost_classification_model/%s'%pair)
# 
#   print('The model of %s has been saved'%pair)

# Commented out IPython magic to ensure Python compatibility.
# %spark delete -s spark_session

"""The result will be exhibited in the next section after being loaded into local Apache Spark. Note that we do **not** tune the hyperparameters for **every** dataframe due to the limitation of time: It takes around 30 minutes to perform a simple grid search as above with a 5-node cluster, given that we are only allowed to use AWS *c5.xlarge* or other less advanced instances, and the time would multiply if we were doing cross validations. Besides, the quickly consumed AWS credit also poses an inevitable problem to this approach. So, taking the time and AWS credit into account, it is decided that we only use the grid search to get a ballpark estimate of the hyperparameters. In retrospect, the estimated hyperparameters are good enough to be used to build useful XGBoost models.

### **4.2** Produce models with local Apache Spark

This section will train 1,000 XGBoost models for the 1,000 dataframes we have stored in our Amazon S3 bucket. First, we install **Apache Spark** and other toolkits for connecting to Amazon Web Service. Still, please ensure the AWS CLI credentials file is in the **/content** directory in Colab before running the following code block.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ! pip install boto3
# ! pip install awscli
# ! pip install s3fs
# ! mkdir ~/.aws
# ! cp -f /content/credentials ~/.aws/
# ! chmod 600 ~/.aws/credentials
# ! apt-get install gcc python-dev libkrb5-dev openjdk-8-jdk-headless
# ! wget https://downloads.apache.org/spark/spark-3.2.0/spark-3.2.0-bin-hadoop3.2.tgz
# ! tar xf spark-3.2.0-bin-hadoop3.2.tgz
# ! pip install findspark
# ! pip install pyspark --upgrade
# ! pip install git+https://github.com/sllynn/spark-xgboost.git#egg=spark-xgboost
# ! mkdir /content/classification_data
# ! aws s3 cp --recursive s3://cis545-group48/classification_data/ /content/classification_data/

"""The next part to fetch coin pairs to be processed, which is no different from its counterpart in the previous section. But this time it will fetch the result to the **Colab** rather than an EC2 instance."""

import boto3

s3 = boto3.client('s3')

prefix = 'classification_data/'
suffix = '.parquet'

lp = len(prefix)
ls = len(suffix)

coin_pairs = []
for page in s3.get_paginator('list_objects_v2').paginate(Bucket='cis545-group48',Prefix=prefix,Delimiter='/'):
    coin_pairs.extend([obj['Key'][lp:-ls] for obj in page.get('Contents',[]) if len(obj['Key'])>lp])

prefix = 'xgboost_classification_model/'
suffix = '/'

lp = len(prefix)
ls = len(suffix)

processed = []
for page in s3.get_paginator('list_objects_v2').paginate(Bucket='cis545-group48',Prefix=prefix,Delimiter='/'):
    processed.extend([sub['Prefix'][lp:-ls] for sub in page.get('CommonPrefixes',[]) if len(sub['Prefix'])>lp])

unprocessed = list(set(coin_pairs)-set(processed))

print(processed)
print(unprocessed)

"""Next, we start the **Apache Spark** and equip it with the correct *.jar* packages."""

import os
from pyspark.sql import SparkSession
import findspark

os.environ['JAVA_HOME']='/usr/lib/jvm/java-8-openjdk-amd64'
os.environ['SPARK_HOME']='/content/spark-3.2.0-bin-hadoop3.2'

findspark.init('/content/spark-3.2.0-bin-hadoop3.2')

spark=(
    SparkSession
       .builder
       .appName('Project')
       .master("local[*]")
       .config('spark.jars','''
          https://repo1.maven.org/maven2/ml/dmlc/xgboost4j_2.12/1.5.1/xgboost4j_2.12-1.5.1.jar,
          https://repo1.maven.org/maven2/ml/dmlc/xgboost4j-spark_2.12/1.5.1/xgboost4j-spark_2.12-1.5.1.jar
       ''')
       .getOrCreate()
)

"""Before starting to produce XGBoost models, we may want to first check our **estimated hyperparameters** from the previous section. Taking the fitted model for APPC-BTC data as an example, the result is shown as follows."""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ! aws s3 cp --recursive s3://cis545-group48/xgboost_classification_model/APPC-BTC /content/xgboost_classification_model/APPC-BTC

from sparkxgb import XGBoostClassificationModel
tuned_model = XGBoostClassificationModel().load('/content/xgboost_classification_model/APPC-BTC')
print(tuned_model.explainParam('numRound'))
print(tuned_model.explainParam('maxDepth'))

"""In this case, our previous **grid search** has picked a *numRound* of 150 and a *maxDepth* of 10! This is really unfortunate for us because we cannot afford such costly training for every one of the 1,000 dataframes. Therefore, we have to consider a **tradeoff** between model performance and training efficiency. In the following training part, we will use a *numRound* of 100 and a *maxDepth* of 5, which at least makes it possible to finish the training in Colab environment. Certainly, it is not as good as using the optimal hyperparameters all the time, but we need to compromise the accuracy a little bit to satisfy the time limitation.

Now the training part is much like training in the cloud environment. But this time, we also evaluate our model on the *test* subset and save the result of evaluation to the **/xgboost_classification_evaluation** directory in the S3 bucket.
"""

from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.sql.functions import udf, col
from pyspark.sql.types import DoubleType
from sparkxgb import XGBoostClassifier
import numpy as np

list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())
extract_prob_udf = udf(lambda v: float(v[1]), DoubleType())

for pair in unprocessed:
  data_sdf = spark.read.load(path='/content/classification_data/%s.parquet'%pair,format='parquet')

  train_sdf, test_sdf = data_sdf.select(
      col('label'),
      list_to_vector_udf(col('features')).alias('features')
  ).randomSplit([0.8, 0.2],seed=2021)

  xgb = (
    XGBoostClassifier(maxDepth=5, numRound=100)
    .setFeaturesCol('features')
    .setLabelCol('label')
  )

  xgb_model = xgb.fit(train_sdf)

  xgb_model.write().overwrite().save('/content/drive/MyDrive/xgboost_classification_model/%s'%pair)
  os.system('aws s3 cp --recursive /content/xgboost_classification_model/%s s3://cis545-group48/xgboost_classification_model/%s'%(pair,pair))

  res_sdf = xgb_model.transform(test_sdf).select(
      extract_prob_udf(col('probability')).alias('probability'),
      col('prediction'),
      col('label')
  )
  res_df = res_sdf.toPandas()
  res_df.to_parquet('s3://cis545-group48/xgboost_classification_evaluation/%s.parquet'%pair)

  print('pair saved: '+pair)

"""The code blocks above will be repeatedly run across several days, and we use S3 bucket to store the progress. After all 1,000 models have been trained, we download their predictions to a mounted *Google drive*. These **prediction dataframes** only contain necessary information for evaluation, so they are small enough to be fitted into a *Google drive* and later handled by *Pandas*."""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ! aws s3 cp --recursive s3://cis545-group48/xgboost_classification_evaluation /content/drive/MyDrive/xgboost_classification_evaluation

"""### **4.3** Analysis of Predictions

Since the training is not done in one sitting, we first want to verify that it is **complete** and that the **total number of prediction dataframes** is exactly **1,000**.
"""

import os

eval_dir = '/content/drive/MyDrive/xgboost_classification_evaluation/'
files = os.listdir(eval_dir)
len(files)

"""Taking the coin pair of **BTC-USDT** as an example, below is what a single prediction dataframe looks like."""

import pandas as pd

pd.read_parquet(eval_dir+'BTC-USDT.parquet').head(10)

"""We can then collect the overall **accuracy** statistics and plot **the accuracy distribution of the 1,000 predictions on 1,000 trading pairs**."""

import seaborn as sns

acc = []
for file in files:
  pred_df = pd.read_parquet(eval_dir+file)
  acc.append(len(pred_df[pred_df['prediction']==pred_df['label']])/len(pred_df))

sns.displot(acc, kde=True, 
             color='darkblue', 
             edgecolor='gray',
             linewidth=2)

"""Seemingly, our result is not that interesting: The accuracies are majorly in the range of **70% to 75%**. Apart from **XGBoost**, other machine learning methods are yielding similar accuracies (as one can see from the previous section where models like Neural Network were applied to a single dataframe). Naturally, we want to question ourselves at this point: Are there any factors making our models give unreliable predictions?

Actually, it is not hard to find one. Let us take a step back and look at what we have attempted to achieve in this whole process. We have thrown a classification problem to a machine and asked it to do the classification for us. The problem is delicately designed to benefit from the balancedness of samples. However, the side effect of using a single value as the categorization boundary is that the categories become more vague and sometimes harder to distinguish. To see this, consider the case where the **acutal volatility** is 0.501 and the model has to **predict** whether or not the volatility would be above 0.5. Such negligible difference makes the classification difficult even in human beings' eyes! Sadly, there are a great many hard samples like this in our dataset since the data points tend to cluster around their medians (much like in a normal distribution), and models usually have a hard time classifying them correctly.

Knowing a crucial source of the inaccuracy, we may reflect on how we tackle this as humans: If a person was presented a very ambiguous sample and they had to give a response, they would possibly make a guess with very **low confidence**. Machine learning models do that too! Recall that we have the **probabilty** values at our disposal, which basically show how **confident** the models are in their predictions. Why not look more closely at the confident results, and discard the random guesses?

The following code blocks show the distributions of accuracies when I enforce a confidence threshold of 0.3 (only keep predictions with probabilties below 20% or above 80%) or 0.4 (only keep predictions with probabilties below 10% or above 90%).
"""

import numpy as np

def plot_confident(threshold):
  acc = []
  for file in files:
    pred_df = pd.read_parquet(eval_dir+file)
    pred_confident_df = pred_df[np.abs(pred_df['probability']-0.5)>threshold]           # filter out predictions with prob. in [0.5-threshold, 0.5+threshold]
    acc.append(len(pred_confident_df[pred_confident_df['prediction']==pred_confident_df['label']])/len(pred_confident_df))

  sns.displot(acc, kde=True, 
              color='darkblue', 
              edgecolor='gray',
              linewidth=2)

plot_confident(0.3)

plot_confident(0.4)

"""Now the result is much better. Is this approach practical in the finance industry? Actually, yes. Traders sometimes call this **filtering out fake signals**. They do it with two rationales:

1. In practice, usually there are many models stacked together, and every model has its own "niche". We want to cherry-pick the model outputs so that we are not misguided by some **random guesses** generated by one specific model

2. More importantly, traders only trade when they have **confidence** in their predictions, and in other time, they are justing waiting for an opportunity to come. This means that we do **not** need predictions all the time

In retrospect, filtering by **probability** is certainly not the only way of signal filtering. People can devise a diversity of strategies on how to utilize the model outputs, but this requires a lot of expertise in finance which we are not proficient in. Therefore, let us stop here and have a review at what we have done in this notebook.

## **Chapter 5**: Review & Conclusion

In this project, we have studied on the [Binance full history data](https://www.kaggle.com/jorijnsmit/binance-full-history) from Kaggle. This is typical time-series-based **big data** on which there are very few previous researches. After preparing the data, we first express it in an intuitive way and try to obtain some financial insights into it. Then we try to predict and classify future volatilties by applying machine learning models to the data: We train a variety of models on a single dataframe to have a taste of how they work in general, before using the delicately selected **XGBoost** model, a highly parallelizable machine learning model, to tackle the whole dataset with the help of **Amazon EMR Cluster** and **Apache Spark**. After the process of **singal filtering**, the predictive results given by our **XGBoost** models have an accuracy of **around 90%** and thus can be of much practical use in the aforementioned fields. During the entire process, we have cautiously avoided some common mistakes that we observed on Kaggle.

After the exploration of a diversity of models, it is natural to consider stacking them together as a more complicated model. Unfortunately, this requires much more computational resources than what we have at our disposal, but surely it is **a good direction for future researches**. On the other hand, perhaps our approach can be **generalized** to model not only **cryptocurrency** data, but also other financial data including foreign exchange rates, stock prices, etc. Hopefully, our research can shed some light on how to predict the volatilities of financial time series in the future.
"""